# 인공지능

- 개요
  - SW. 지능적인 행동의 자동화. 인간의 지능적, 반복적인 행동을 수행
  - 인공지능의 90% 이상은 지도학습(Supervised Learning)
  - 인공지능에 대한 스스로 똑똑해지거나 감정을 가질 수 없음

- 기타
  - 모라벡의 역설 : 사람에게 쉬운것이 기계에게 어렵고, 기계에게 어려운것이 사람에게 쉽다.
  - 배경 : 이미 이론은 있었음 양질의 데이터를 확보할 수 있는 환경과 GPU를 넘는 TPU칩 등의 하드웨어 환경이 갖춰짐
  - 2차원 matrix, 다차원 tensor
    - 차원 = 채널 = depth
  - 유명한 이미지 인식 대회(ImageNet Large Scale Visual Recognition Competition)의 AlexNet이 CNN의 전환기
  - 도메인 적응 문제 : 추론 환경이 다르면 AI는 제 기능을 수행하지 못함

- 강인공지능 : 스스로 판단하고 결정하는 영화에서 말하는 인공지능
  - 범용성을 가지는 범인공지능
- 약인공지능 : 제한된 환경의 특정 업무 수행을 위한 인공지능
  - 알파고, 서빙로봇 같은 특화. 범용성의 폭이 좁음

- 인공지능 > 머신러닝 > 딥러닝
  - 머신러닝 : 데이터로부터 패턴을 기계가 스스로 학습
  - 딥러닝 : 인공신경망 기반. 비정형 raw에서부터 특징 추출과 판단까지 수행

- 정형데이터(structure) : 안정성. 체계적. 구조고정.
  - 머신러닝기반의 빅데이터 분석기법: R, SAS, SPSS 등의 통계분석 틀
  - 예시
    - 선형/로지스틱 회귀분석을 통한 실수 예측
    - 의사결정나무를 통한 카테고리분류
    - ARMA, ARIMA 모형을 통한 시계열 예측
- 비정형데이터 : 다양한 형식. 텍스트, 음성, 이미지, 영상.
  - 생산 데이터의 대다수고 비율이 계속 증가
  - 딥러닝 기반의 데이터 처리

- 인공신경망(Artificial Neural Network) : 인간처럼 기계에게 사례를 통한 학습을 시켜보자
  - 인공뉴런 : 데이터를 받아 가중합 연산을 하여 다음 인공뉴런으로 데이터를 넘김
  - 인공뉴런을 여럿 쌓아 인공신경망을 만들어 인간의 사고방식을 재현
  - 기계가 특징을 파악하여 분류를 수행
- CNN(Convolutional Neural Network) : 이미지처리를 위한 특별한 인공신경망 구성 방식
  - 특징 추출(Feature Extraction) 영역 + 태스크 수행 영역
    - 특징 추출
      - 컨볼루션 연산을 통한 이미지 특징 추출 및 Feature map 도출
      - 풀링 연산 : Feature map에서 핵심 정보 sampling
        - 주로 가장 큰 값만을 가져오는 max pool 방식
    - 태스크 수행 : 대표적 예시 몇 가지
      - Classification : 입력 이미지를 K개의 클래스(카테고리)로 분류
      - Detection : 특정 개체의 위치와 좌표값을 네모박스 단위로 찾아 줌
      - Segmentation : 개체를 픽셀 단위로 찾고 영역을 구별

- 자연어이해(NLU;Natural Language Understanding/Processing)
  - 인공어 : 자연어 반댓말 의도와 목적에 따라 만들어진 언어. 프로그래밍 언어. 기계 언어. 에스페란토.
  - 이해 : 맥락 내용의 파악. 규칙에 따른 수행인 처리(Processing)보다 고차원
  - Tokenizing : 한 덩이 문장을 인공신경망이 인식할 수 있는 세부단위로 Parsing
    - 목적에 따라 어절, 형태소, 음절(교착어인 한국어에 적당), 자소 등 여러 방식으로 나뉨
  - 워드임베딩 : 토큰을 벡터화
    - 원-핫 인코딩 : 단어대로 번호를 붙임(이때 숫자간 우열은 없음)
    - CBOW와 SKIPGRAM : 문맥을 통해 단어의 뜻을 알아내는 알고리즘. 벡터길이가 원핫인코딩보다 훨씬 작음(51만->300)
      - CBOW : 문장에 빈칸을 만들어 단어(토큰)를 유추시킴
      - SKIPGRAM : 단어를 하나 알려주고 주위에 등장할만한 문맥을 만들게 함
  - 과제
    - 문장/문서분류
    - Sequence-to-Sequence : 번역, 요약
    - 질의 응답 : IR(Information Retrieval) : 과거 유사답변, MRC : 매뉴얼 내 가능성 높은 영역을 반환

- 시계열데이터 처리
  - 순환신경망(RNN; Recurrent Neural Network) : 과거 데이터 처리 과정일부를 가져와 현 시점에 반영하는 인공신경망
    - 벡터 형태로 정보(feature)를 넘김. 정보의 누적이 인코딩, 결과의 출력이 디코딩
    - 장점 : 과거 정보 반영, 가변길이 데이터 처리, 다양한 구조로 구출
    - 단점 : 느림, 불안정한 학습, 폭발적 학습량 증가(Gradient Exploding), 여러번 압축된 과거 정보의 활용(Gradient Vanishing)
    - 성능보완(Long-Short Term Memory) : forget(잊어버림), input(현재 정보), output(나중에 얼마나 쓸지) / GRU와 유사

- AI 도입 과정
  - Offline Process : Training Pipeline. 데이터 가공, 정제, 라벨링
    - 튜닝 : 좋은 성능이 나올때까지 반복 실험, 하이퍼파라미터 설정
    - model : Validate&Select > Train > Publish(최종 선택)
  - Online Process : 추론 하여 실제환경에서 운영
  - 주의사항 : 훈련시 데이터에만 잘 작동하는 Overfitting
    - Training, Validation, Test 추천 비율; 8:1:1, 6:2:2
    - Regularization : 일반화 성능을 향상시켜 Overfitting 개선하는 기법
      - 데이터 증강 : 반전, 크롭, 노이즈, 색상, 명암, 채도 등으로 데이터를 늘림
      - Capacity : 모델 복잡 정도. 늘리면 학습을 잘하다 못해 암기해버림
      - 조기 종료 : 학습 곡선이 어느 정도 올라가면 학습을 종료해버림
      - 드롭 아웃 : 교육시 몇몇 인공 뉴런을 종료해버림

- 전이 학습(Transfer Learning) : 한번 학습한 딥러닝 모델을 재활용하는 기법
  - Fine-Tuning : 유사어. 파라미터 튜닝을 통한 재학습
  - Catastrophic forgetting : 새 정보 학습 중 기존 정보를 잊는 경향
    - 레이어 동결 : 전반부 파라미터를 학습시키지 않고 고정시킴
    - Discriminative fine-tuning : Leaning Rate(학습률)에 차별
  - Transfer Learning 모델이용 : 기학습 모델을 특정 도메인 태스크에 적용
  - 사전학습모델 : 전이학습을 염두에 두고 활용할 수 있는 모델을 미리 만들어 둠

- 데이터가 많지만 '학습시킬 데이터'가 적을때
  - 자가지도학습 : 시스템이 자체 라벨을 만들어서 사용
    - BERT : 자연어 연구에 있어 특정 분야가 아닌 언어라는 분야 전반에 걸쳐 지식을 두루 쌓게 학습
  - 능동학습(Active Learning) : 라벨 데이터 수가 제한적일때 효과적인 데이터를 선별하여 성능을 극대화
    - 전문성이 필요한 도메인이나 데이터 생성을 위한 비용이 클 때
    - 일부 라벨링 모델 학습 > 학습에 도움되는 데이터 선별 > 해당 데이터만 라벨링 > 병합하여 다시 학습
    - 쿼리 전략(Query Strategy) : 효과적인 데이터를 선별하는 방법
      - 모델 학습시 생긴 경계 주위의 데이터만 사용한다던가
      - Uncertainty Sampling : 애매한 데이터를 우선적으로 라벨링
      - Query by committee : 여러 AI 모델간 의견 불일치가 클 수록 라벨링

- Attention mechanism : 어떤 부분이 의사결정에 중요한지 집중
  - Attention score : 집중가중치 0~1 -> Context Vector : 문맥 정보를 반영
  - 디코딩할때마다 과거의 데이터 특징(feature)을 고려할 수 있게 됨
  - 기계가 어느 부분을 중요하게 판단하였는지 알 수 있게 해줌 > 설명할 수 있는 AI(XAI)
  - Transformer : 입력 데이터간 self-attention하여 상호 정보교환하는 인공신경망

- AutoML : 스스로 진화하는 인공지능
  - Feature Engineering : 특징을 선택하고 인코딩 방식을 정함
  - 하이퍼파라미터 자동탐색 : 그리드서치, 랜덤서치 > 메타 러너 : 하이퍼파라미터 탐색을 위한 RNN. Learn to Learn
  - 아키텍쳐 탐색 : NAS(Neural Architecture search) : 메타러너와 러너를 조합
  - AWS, Azure, GCP에서 AutoML 수행을 위한 서비스 제공

- XAI(설명가능한 인공지능)
  - 배경 : 딥러닝은 왜 이런 결과가 도출되었는지를 알기 힘듬. 개선에도 힘듬
  - 방식
    - 어텐션 메커니즘 활용
    - 설명하는 법을 학습하는 모델을 만들기
